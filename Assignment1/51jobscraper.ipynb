{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request# this step helps decode and then encode the original page later \n",
    "import bs4\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_page(url):  \n",
    "    r = urllib.request.urlopen(url) # these two steps helps solve the error I met when the original page cannot be read in utf-8 codes\n",
    "    r1 = r.read().decode('gbk')# read the original page and 'translate' it to gbk codes\n",
    "    mypage=BeautifulSoup(r1)\n",
    "    \n",
    "    jobs = [a.attrs.get('title') for a in mypage.select('p.t1 a')]\n",
    "    company = [a.attrs.get('title') for a in mypage.select('span.t2 a')]\n",
    "    base = [span.text for span in mypage.select('span.t3')]\n",
    "    del base[0] #this is because theere was a headline of t3-t5 column, so I dropped them here\n",
    "    salary = [span.text for span in mypage.select('span.t4')]\n",
    "    del salary[0]\n",
    "    time = [span.text for span in mypage.select('span.t5')]\n",
    "    del time[0]\n",
    "    \n",
    "    page_content=[]\n",
    "    for i in range (0,len(jobs)):\n",
    "        page_content.append([jobs[i],company[i],base[i],salary[i],time[i]])\n",
    "    \n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list=[]\n",
    "for i in range(0,42):\n",
    "    page_number=str(i+1)\n",
    "    url = 'https://search.51job.com/list/010000%252C020000%252C030200%252C040000%252C080200,000000,2500%252C3300%252C3404%252C4300%252C4500,32%252C03%252C13%252C48%252C43,3,99,%2B,2,' + page_number +'.html?lang=c&stype=1&postchannel=0000&workyear=99&cotype=99&degreefrom=05&jobterm=01&companysize=99&lonlat=0%2C0&radius=-1&ord_field=0&confirmdate=9&fromType=22&dibiaoid=0&address=&line=&specialarea=00&from=&welfare='\n",
    "    url_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "information=[]\n",
    "for spot in url_list:\n",
    "    my=get_the_page(spot)\n",
    "    information.extend(my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('51jobs.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)#this means use ',' as seperator, '\"' as quote mark, and only quote those fields which contain special characters such as delimiter, quotechar or any of the characters in lineterminator \n",
    "    header= ['职位','公司名','地点','薪酬','发布时间']\n",
    "    spamwriter.writerow(header)\n",
    "    spamwriter.writerows(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
